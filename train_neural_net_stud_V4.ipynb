{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAjSl9PAcYoM"
   },
   "source": [
    "# Mini-Challenge 1 : Train a Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEwT_tt-cYoQ"
   },
   "source": [
    "### Goals\n",
    "\n",
    "Apply plain vanilla gradient descent to a relatively simple neural network. \n",
    "The network can be used to train a binary classification task such as distinguishing whether a cat is seen on a given image or not. \n",
    "\n",
    "For the implementation, a cost function is defined (on the training dataset) and the gradient w.r.t. the parameters of the model needs to be calculated. \n",
    "\n",
    "The network will be a fully connected neural network (multi-layer perceptron, MLP) with a single hidden layer. The implementation should be capable of dealing with differently shaped input datasets (input size of flattened image $n_0$) and number of units in the hidden layer ($n_1$). This will allow to apply your implementation to two different datasets, MNIST and a dataset with/without cats. \n",
    "\n",
    "**Overview**\n",
    "\n",
    "| Problem | Content | Points |\n",
    "| :--- | :--- | :---: |\n",
    "| Problem 1 | Derive Formulas | 8 |\n",
    "| Problem 2 | Implement Model and Cost Function and its Gradients | 8 |\n",
    "| Problem 3 | Implement Gradient Descent Training | 8 |\n",
    "| Problem 4 | Implement Gradient Checking | 6 |\n",
    "| Problem 5 | Apply GD to MNIST | 4 |\n",
    "| Problem 6 | Apply GD to Cats/NoCats | 4 |\n",
    "| Total | | 38 |\n",
    "\n",
    "\n",
    "Please send your solution per mail to [Martin](mailto:martin.melchior@fhnw.ch) and [Stefan](mailto:stefan.hackstein@fhnw.ch).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wJr9hCjcYoS"
   },
   "source": [
    "## Introductionary Remarks: Imports, Plotting\n",
    "\n",
    "Implement the model as described below with <code>Numpy</code>. \n",
    "For plotting use <code>Matplotlib</code>. \n",
    "\n",
    "Hence, you will only need the following imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "90ztI84ocYoT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from types import FunctionType ## this is only used to indicate the expected type of function arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzfAh8x2cYoU"
   },
   "source": [
    "For tracking progress of the optimization, you will plot the cost as a function of the iteration (or \"epoch\"). For this purpose, you can use the method <code>learningcurve_plots</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jaODjfvecYoV"
   },
   "outputs": [],
   "source": [
    "def learningcurve_plots(cost_hist: np.array, learning_speed_hist: np.array, logy: bool = False):\n",
    "    \"\"\"\n",
    "    cost_hist -- history of cost values, as numpy-array of shape (T,1)\n",
    "    learning_speed_hist -- history of learning speed values, as numpy-array of shape (T,1)\n",
    "    logy -- if set to True will plot the y axis at logarithmic scale\n",
    "    \"\"\"\n",
    "    plt.figure(1)\n",
    "    T = len(cost_hist)\n",
    "    if logy:\n",
    "        plt.semilogy(np.arange(T),cost_hist,'b-')\n",
    "    else:\n",
    "        plt.plot(np.arange(T),cost_hist,'b-')\n",
    "    plt.title(\"Cost\")\n",
    "        \n",
    "    plt.figure(2)\n",
    "    T = len(learning_speed_hist)\n",
    "    if logy:\n",
    "        plt.semilogy(np.arange(T),learning_speed_hist,'g-')\n",
    "    else:\n",
    "        plt.plot(np.arange(T),learning_speed_hist,'g-')\n",
    "    plt.title(\"Learning Speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxFO_PgjcYoW"
   },
   "source": [
    "## Problem 1 : Derive Formulas for Model and Cost Function and Gradient\n",
    "\n",
    "### Problem 1a): Model Function and Gradient\n",
    "\n",
    "Here, you should mathematically describe the model function and its gradient. \n",
    "Actually, I will give you the structure of the model, you will then write down the components of the gradients.\n",
    "\n",
    "As described above, the model contains a single hidden layer with $n^{(h)}$ units and a single input is given by an array of length $n^{(in)}$.\n",
    "To compute the output from the input $x^{(in)}$, the following formulas hold:\n",
    "\n",
    "$a^{(h)} = \\sigma\\left(W^{(h)}\\cdot x^{(in)} + b^{(h)}\\right)$<br>\n",
    "$a^{(out)} = \\sigma\\left(W^{(out)}\\cdot a^{(h)} + b^{(out)}\\right)$\n",
    "\n",
    "where \n",
    "* $W^{(h)}$ is a $n^{(h)}\\times n^{(in)}$-matrix, \n",
    "* $b^{(h)}$ is a $n^{(h)}\\times 1$-matrix, \n",
    "* $W^{(out)}$ is a $1\\times n^{(h)}$-matrix, \n",
    "* $b^{(out)}$ is a scalar \n",
    "\n",
    "and $\\sigma(z)=\\frac{1}{1+\\exp(-z)}$ is the sigmoid function which is applied element-wise if the input is a vector or matrix. The output of the model $a^{(out)}$ is a scalar. Note that the model output is a function of the input $x$ and the model parameters summarized as $\\theta$, i.e. \n",
    "\n",
    "$$a^{(out)}=y^{(pred)}(x^{(in)},\\theta)$$\n",
    "\n",
    "Derive the formulas for the partial derivatives with respect to the components of the parameters $\\theta$, i.e. of the components of $W^{(h)}, b^{(h)}, W^{(out)}, b^{(out)}$.\n",
    "Define a scheme that makes efficient use of intermediate results, i.e. make manifest the terms that can be re-used for computing the parameters of the output and the hidden layer as well as the terms that can be reused to compute the weights and the bias for the output or the hidden layer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a^{(out)} = y^{(pred)}(x^{(in)}, \\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a^{(out)}=y^{(pred)}(x^{(in)}, W^{(h)}, b^{(h)}, W^{(out)}, b^{(out)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a^{({out})}=y^{(pred)}(x^{(in)},\\theta)=\\sigma\\left( W^{{(out)}} \\cdot\n",
    "\\sigma\\left(W^{(h)} x^{(\\text{in})}+b^{{(h)}}\\right) +b^{{(out)}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial y^{({pred})}}{\\partial x^{({in})}}=\n",
    "\\sigma\\left(\\sigma (W^{(h)} x^{({in})} + b^{({h})}) \\cdot \n",
    "W^{({out})} + b^{({out})}\\right) \\cdot\n",
    "\\left(1 - \\sigma(\\sigma (W^{(h)} x^{({in})} + b^{({h})}) \\cdot \n",
    "W^{({out})} + b^{({out})})\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma\\left(W^{(h)} x^{({in})}+b^{(h)}\\right) \\cdot \\left(1 - \\sigma(W^{(h)} x^{({in})}+b^{(h)})\\right) \\cdot\n",
    "W^{{(h)}} \\cdot W^{{(out)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial y^{({pred})}}{\\partial x^{({in})}}= y^{({pred})} \\cdot (1 - y^{({pred})}) \\cdot\n",
    "a^{({h})} \\cdot (1 - a^{({h})}) \\cdot W^{({h})} \\cdot W^{({out})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial y^{({pred})}}{\\partial W^{({h})}}=\n",
    "\\sigma^{\\prime}\\left(\\sigma (W^{(h)} x^{({in})} + b^{({h})}) \\cdot W^{({out})} + b^{({out})}\\right) \\cdot \\sigma^{\\prime}\\left(W^{(h)} x^{({in})}+b^{(h)}\\right) \\cdot x^{{(in)}} \\cdot W^{{(out)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial y^{({pred})}}{\\partial W^{({h})}}= y^{({pred})} \\cdot (1 - y^{({pred})}) \\cdot W^{({out})} \\cdot x^{({in})} \\cdot a^{({h})} \\cdot (1 - a^{({h})})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial y^{({pred})}}{\\partial b^{({h})}}=\n",
    "\\sigma^{\\prime}\\left(\\sigma (W^{(h)} x^{({in})} + b^{({h})}) \\cdot W^{({out})} + b^{({out})}\\right) \\cdot \\sigma^{\\prime}\\left(W^{(h)} x^{({in})}+b^{(h)}\\right) \\cdot W^{{(out)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial y^{({pred})}}{\\partial W^{({out})}}=\n",
    "\\sigma^{\\prime}\\left(\\sigma (W^{(h)} x^{({in})} + b^{({h})}) \\cdot W^{({out})} + b^{({out})}\\right) \\cdot \\sigma\\left(W^{(h)} x^{({in})}+b^{(h)}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial y^{({pred})}}{\\partial b^{({out})}}=\n",
    "\\sigma^{\\prime}\\left(\\sigma (W^{(h)} x^{({in})} + b^{({h})}) \\cdot W^{({out})} + b^{({out})}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utxAqJKLcYoX"
   },
   "source": [
    "### Probem 1b): Cost Function and Derivatives\n",
    "\n",
    "As cost function, we use the binary cross-entropy loss applied to the model prediction $y^{(pred)}=a^{(out)}$ defined to be\n",
    "\n",
    "$$L(y^{(pred)}, y) = -y\\log(y^{(pred)})-(1-y)\\log(1-y^{(pred)})$$\n",
    "\n",
    "Here, $y$ denotes the binary label associated with the input, i.e. $y\\in\\{0,1\\}$ (e.g. '0' - image contains not cat | '1' - image contains cat). For a given input $x^{(in)}=x$ we use the model with parameters $\\theta$ to predict the output $a^{(out)}=y^{(pred)}(x,\\theta)$ and then compute the loss according to the formula above. \n",
    "\n",
    "The cost function is defined as the loss function averaged over all the dataset, i.e.\n",
    "\n",
    "$$C(\\theta) = \\frac{1}{N}\\sum_{i=1}^N\\,L\\left(y^{(pred)}(x^{(i)},\\theta), y^{(i)}\\right)$$\n",
    "\n",
    "We now use gradient descent to determine the parameters at which the cost function becomes minimal.\n",
    "\n",
    "When computing the gradient of this cost w.r.t. model parameters, you need to compute the derivative of the cross entropy loss function, $L$ w.r.t. $y^{(pred)}$  and apply the chain rule.\n",
    "\n",
    "Derive the formula for the derivative of $L$ w.r.t. $y^{(pred)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L{}}{\\partial y^{({pred})}} =  -y\\cdot\\frac{y^{\\prime\\space{({pred})}}}{y^{({pred})}} - {({1-y})} \\cdot \\frac{-y^{\\prime\\space{({pred})}}}{1-y^{({pred})}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L{}}{\\partial y^{({pred})}} =  -y\\cdot\\frac{y^{({pred})}\\cdot(1-y^{({pred})})}{y^{({pred})}} - {({1-y})} \\cdot \\frac{-y^{({pred})}\\cdot(1-y^{({pred})})}{1-y^{({pred})}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L{}}{\\partial y^{({pred})}} =  -y\\cdot(1-y^{({pred})}) - {({1-y})} \\cdot (-y^{({pred})})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sofern $\\log$ = $\\ln$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5zowvZYcYoZ"
   },
   "source": [
    "## Problem 2 : Implement Model and Cost Function and its Gradient\n",
    "\n",
    "Here, you translate the formulas of problem 1 into code. \n",
    "\n",
    "### Problem 2a): Model\n",
    "\n",
    "For convenience, we define the model as a python class which allows to keep the parameters and intermediate results (such as $a^{(h)}$) as member variables.\n",
    "\n",
    "The class should have the following instance methods:\n",
    "* $forwardprop(x)$ : Compute the model output when passing an input $x$ through the model. Implement it such that you can handle multiple inputs at the same time, i.e. assume $x$ is a numpy array of shape $(n^{(in)},N)$ where $N$ is the number of inputs.\n",
    "* $backwardprop(\\dots)$ : Compute the gradient with respect to the model parameters. \n",
    "* $initialize(\\dots)$ : Set initial values for the model parameters.\n",
    "* $update\\_params(\\dots)$: Update the model parameters in accordance with the gradient descent update rule.\n",
    "\n",
    "Make efficient use of the scheme derived in problem 1 by keeping intermediate results as member variables. Assume that $backwardprop$ is called after $forwardprop$ has been invoked. Introduce suitable member variables for the intermediate variables needed for the computation of the gradients. Furthermore, introduce member variables for the parameters (weights, bias) and their gradients.  \n",
    "\n",
    "Assume the following shapes for the quantities involved ($n^{(in)}=n0, n^{(h)}=n1$):\n",
    "* $W^{(h)}$ : 2d numpy array of shape (n1,n0)\n",
    "* $b^{(h)}$ : 1d numpy array of shape (n1,1)\n",
    "* $W^{(out)}$ : 1d numpy array of shape (1,n1)\n",
    "* $b^{(out)}$ : scalar quantity \n",
    "* $x^{(in)}$ : 2d numpy array of shape (n0,N)\n",
    "* $a^{(out)}$ : 2d numpy array of shape (1,N)\n",
    "\n",
    "For the gradients, there will be a contribution for each input which needs to be properly averaged. The resulting gradient should have the same dimension as the parameter arrays. The input to the $backwardprop$ function is fed by the derivative of the loss function (evaluated element-wise per sample). This will allow to compute gradients with respect to the parameters properly averaged over all the samples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "T-tShhU2cYoa"
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, ninput: int, nhidden: int):\n",
    "        \"\"\"\n",
    "        Constructor of the model designed for given input dimension and number of units in hidden layer.\n",
    "        \n",
    "        Arguments:\n",
    "        ninput - size of input\n",
    "        nhidden - size of hidden layer\n",
    "        \"\"\"\n",
    "        self.ninput = ninput\n",
    "        self.nhidden = nhidden\n",
    "        \n",
    "    def _sigmoid(self, z: np.array) -> np.array2string:\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def initialize(self, initial_weights_hidden: np.array, initial_weights_output: np.array):\n",
    "        \"\"\"\n",
    "        Initialize the weights of the model with given input arguments and reset the biases to zero values. \n",
    "        \n",
    "        Arguments:\n",
    "        initial_weights_hidden - initial weights of the hidden layer (must be of shape (nhidden, ninput))\n",
    "        initial_weights_output - initial weights of output layer (must be of shape (1, nhidden))\n",
    "        \"\"\"\n",
    "        assert (self.nhidden,self.ninput)==initial_weights_hidden.shape\n",
    "        assert (1,self.nhidden)==initial_weights_output.shape\n",
    "        self.weights_hidden = initial_weights_hidden\n",
    "        self.weights_output = initial_weights_output\n",
    "        self.bias_hidden = np.zeros((self.nhidden,1), dtype=float)\n",
    "        self.bias_output = 0.0\n",
    "        \n",
    "    def forwardprop(self, x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Computes the forward path through the model. Keeps as state variables the inputs and the activations of the hidden layer and the output layer \n",
    "        (as numpy arrays of shape (n0,N), (n1,N), (1,N)). The activations of the layer are finally returned from the method call.\n",
    "        \n",
    "        Arguments:\n",
    "        x - shape (n0, N)\n",
    "        \n",
    "        Returns:\n",
    "        output of the model, numpy array of shape (1,N)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.a_h = self._sigmoid(self.weights_hidden @ x + self.bias_hidden)\n",
    "        self.a_out = self._sigmoid(self.weights_output @ self.a_h + self.bias_output)\n",
    "        return self.a_out\n",
    "\n",
    "    def backwardprop(self, deriv_loss: np.array):\n",
    "        \"\"\"\n",
    "        The computation is done after running the forwardprop. It keeps the gradient components with respect to the different \n",
    "        parameters in suitable arrays: \n",
    "        - grad_weights_hidden\n",
    "        - grad_bias_hidden\n",
    "        - grad_weights_output\n",
    "        - grad_bias_output\n",
    "        Note by passing in the derivate with respect to the loss it allows to compute the components of the gradient of the loss.\n",
    "        \n",
    "        Arguments:\n",
    "        deriv_loss - the gradient of the loss w.r.t. output of the model, a numpy array of shape (1,N)\n",
    "        \n",
    "        Returns:\n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "    def backwardprop(self, deriv_loss: np.array):\n",
    "        self.grad_bias_output    = np.mean(deriv_loss * (self.a_out * (1-self.a_out)))\n",
    "        self.grad_weights_output = deriv_loss @ self.a_out.T * ((1-self.a_out) @ self.a_h.T)\n",
    "        self.grad_bias_hidden    = np.mean(deriv_loss * (self.a_out * (1-self.a_out))) * (self.grad_weights_output @ (self.a_h @ (1-self.a_h).T)).T \n",
    "        self.grad_weights_hidden = deriv_loss * self.a_out * (1-self.a_out) * (self.weights_output * (self.a_h * (1-self.a_h)).T).T @ self.x.T\n",
    "\n",
    "\n",
    "\n",
    "    def update_params(self, lr: float):\n",
    "        \"\"\"\n",
    "        Updates the parameters of the model according to gradient descent. It is assumed that the weights are properly initialized and the gradients\n",
    "        previously computed.\n",
    "        \n",
    "        Arguments:\n",
    "        lr - learning rate\n",
    "        \n",
    "        Returns:\n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        # START YOUR CODE\n",
    "        self.weights_hidden -= lr * self.grad_weights_hidden\n",
    "        self.weights_output -= lr * self.grad_weights_output\n",
    "        self.bias_hidden -= lr * self.grad_bias_hidden\n",
    "        self.bias_output -= lr * self.grad_bias_output\n",
    "        # END YOUR CODE\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m np\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_almost_equal(\u001b[38;5;241m0.5575659\u001b[39m, mtest\u001b[38;5;241m.\u001b[39mforwardprop(x), decimal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# backward  ## use convention: gradient.shape == parameter.shape\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mmtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackwardprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# shapes\u001b[39;00m\n\u001b[0;32m     24\u001b[0m np\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_allclose((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m), mtest\u001b[38;5;241m.\u001b[39mgrad_weights_output\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# woutput.shape=(1,3)\u001b[39;00m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mModel.backwardprop\u001b[1;34m(self, deriv_loss)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_bias_hidden    \u001b[38;5;241m=\u001b[39m deriv_loss \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_output \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_weights_output \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_h))\u001b[38;5;241m.\u001b[39mT \n\u001b[0;32m     69\u001b[0m                          \u001b[38;5;66;03m# self.a_out * (1-self.a_out) * self.weights_output * (self.a_h * (1-self.a_h)).T\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_weights_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mderiv_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_bias_hidden\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)"
     ]
    }
   ],
   "source": [
    "# Unit test - following the convention that\n",
    "# shape of gradients w.r.t. weights/biases = shape of weights/biases\n",
    " \n",
    "# Test with a tiny model and one sample\n",
    "mtest = Model(2,3)\n",
    "whidden = np.array([[0.1,0.9],[-0.2,0.7],[1.1,-1]]).reshape(3,2)\n",
    "woutput = np.array([[0.5,-0.3,.1],]).reshape(1,3)\n",
    " \n",
    "# sample\n",
    "x = np.array([1,1]).reshape(2,1)\n",
    " \n",
    "# initialize\n",
    "mtest.initialize(whidden, woutput)\n",
    "np.testing.assert_array_almost_equal_nulp(whidden, mtest.weights_hidden)\n",
    "np.testing.assert_array_almost_equal_nulp(woutput, mtest.weights_output)\n",
    " \n",
    "# forward\n",
    "np.testing.assert_almost_equal(0.5575659, mtest.forwardprop(x), decimal=7)\n",
    " \n",
    "# backward  ## use convention: gradient.shape == parameter.shape\n",
    "mtest.backwardprop(np.array([1,]).reshape(1,1))\n",
    " \n",
    "# shapes\n",
    "np.testing.assert_allclose((1,3), mtest.grad_weights_output.shape) # woutput.shape=(1,3)\n",
    "#mtest.grad_bias_output -> scalar value\n",
    "np.testing.assert_allclose((3,2), mtest.grad_weights_hidden.shape) # whidden.shape=(3,2)\n",
    "np.testing.assert_allclose((3,1), mtest.grad_bias_hidden.shape) # bias_hidden = (3,1)\n",
    " \n",
    "# regression results for the gradients\n",
    "np.testing.assert_allclose(0.24668616, mtest.grad_bias_output)\n",
    "np.testing.assert_array_almost_equal(np.array([0.18034203,0.1535521,0.1295051]).reshape(1,3), mtest.grad_weights_output, decimal=7)\n",
    "np.testing.assert_array_almost_equal(np.array([0.02425072,-0.01739165,0.00615176]).reshape(3,1), mtest.grad_bias_hidden, decimal=7)\n",
    "np.testing.assert_array_almost_equal(np.array([[0.02425072,-0.01739165,0.00615176],[0.02425072,-0.01739165,0.00615176]]).T.reshape(3,2), mtest.grad_weights_hidden, decimal=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tm2TgB6icYoh"
   },
   "source": [
    "### Problem 2b): Cost, Loss \n",
    "\n",
    "Similarly, we define a class for the Cross-Entropy loss function. While the loss has been defined in 1b), the **cost** just takes the average over all data points in the dataset used for training.\n",
    "\n",
    "Implement for the cross-entropy class the instance methods \n",
    "* $cost(...)$\n",
    "* $derivative\\_loss(...)$\n",
    "\n",
    "as described below. Note that these are evaluated for given numpy arrays $ypred$ and $ytrue$ with the model prediction $a^{(out)}$ and the true label $y$, each of shape $(1,N)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdVzJsvJcYoi"
   },
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "\n",
    "    def cost(self, ypred: np.array, ytrue: np.array) -> float:\n",
    "        \"\"\"\n",
    "        Computes the average binary cross entropy cost for given arrays ytrue and ypred, with the ground truth label values (in {0,1}) \n",
    "        and the predicted proabability to observe label 1, respectively.  \n",
    "\n",
    "        Arguments:\n",
    "        ypred - numpy array of shape (1,N) with the predicted values for N data points\n",
    "        ytrue - numpy array of shape (1,N) with the ground truth values for N data points\n",
    "\n",
    "        Returns:\n",
    "        Cost averaged over the samples, i.e. a scalar value.\n",
    "        \"\"\"\n",
    "        # START YOUR CODE\n",
    "        return -1/ytrue.shape[1] * np.sum(ytrue * np.log(ypred) + (1-ytrue) * np.log(1-ypred))\n",
    "        # END YOUR CODE\n",
    "\n",
    "\n",
    "    def derivative_loss(self, ypred: np.array, ytrue: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Computes the derivative of the cross entropy loss function for given true label y and predicted ypred. \n",
    "        We expect numpy arrays to be passed as input arguments, so that the derivative should be computed element-wise. \n",
    "\n",
    "        Arguments:\n",
    "        ypred - numpy array of shape (1,N) with the predicted values for N data points\n",
    "        ytrue - numpy array of shape (1,N) with the ground truth values for N data points\n",
    "\n",
    "        Returns:\n",
    "        Element-wise derivative of the loss function (numpy array of shape (1,N))\n",
    "        \"\"\"    \n",
    "        # START YOUR CODE\n",
    "        return -ytrue * (1-ypred) + (1-ytrue) * ypred\n",
    "        # END YOUR CODE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "print(CrossEntropy().cost(np.array([0.1,0.9]).reshape(1,2), np.array([0,1]).reshape(1,2)))\n",
    "\n",
    "derivative_loss = CrossEntropy().derivative_loss(np.array([0.1,0.9]).reshape(1,2), np.array([0,1]).reshape(1,2))\n",
    "print(derivative_loss)\n",
    "np.testing.assert_allclose((1,2), derivative_loss.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONsY38HPcYoi"
   },
   "source": [
    "## Problem 3: Implement the Gradient Descent Training (GD)\n",
    "\n",
    "Implement the gradient descent update loop for a given instance of the model and cost.\n",
    "\n",
    "In order to monitor the update of the training, keep book at each iteration about \n",
    "* value of the cost (at the given parameters)\n",
    "* learning speed (~length of the gradient) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzNNb2YJcYoj"
   },
   "outputs": [],
   "source": [
    "def vanilla_gradient_descent(model: Model,\n",
    "                             cost: FunctionType, \n",
    "                             weights_hidden: np.array,\n",
    "                             weights_output: np.array, \n",
    "                             data: dict, \n",
    "                             lr: float, \n",
    "                             epochs: int\n",
    "                             ) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs plain vanilla gradient descent for the cost function. The variables x of the function are provided as np-arrays.\n",
    "    \n",
    "    Arguments:\n",
    "    model -- model\n",
    "    cost -- cost function object\n",
    "    weights_hidden -- (initial) weights in the hidden layer\n",
    "    weights_output -- (initial) weights in the output layer\n",
    "    lr -- learning rate\n",
    "    epochs -- maximum number of iterations\n",
    "    data -- dataset as dictionary with the input data (key: \"input\" | value: numpy array of shape (n0,N)) and label data (key: \"label\" | numpy array of shape (1,N)) \n",
    "    \n",
    "    data = {\n",
    "        \"label\": shape ( 1,N),\n",
    "        \"input\": shape (n0,N)\n",
    "    }\n",
    "\n",
    "    Returns:\n",
    "    cost_hist -- history of the values of the cost function seen during the iteration loop: np-array of shape (T,1) where T is the number of iteration needed.\n",
    "    learning_speed_hist -- history of the learning speed where the learning speed given by the norm of the difference between subsequent x-values: np-array of shape (T,1) where T is the number of iteration needed.\n",
    "    \"\"\"\n",
    "    # START YOUR CODE\n",
    "    data = np.load(\"./mnist.npz\")\n",
    "    X = data[\"X\"].T[1:,:]\n",
    "    y = data[\"y\"].T.reshape(1, X.shape[1])\n",
    "    initial_weights = utils.initial_weights(X.shape[0], 50)\n",
    "    weights_hidden = initial_weights[0]\n",
    "    weights_output = initial_weights[1]\n",
    "    data_dict = {\"label\": \"\", \"input\": \"\"}\n",
    "    data_dict[\"label\"], data_dict[\"input\"] = y, X\n",
    "    def vanilla_gradient_descent(model, cost_obj, weights_hidden: np.array, weights_output: np.array, data: dict, lr: float, epochs: int) -> tuple:\n",
    "        model.initialize(weights_hidden, weights_output)\n",
    "        N = len(data[\"input\"])\n",
    "        for i in range(epochs):\n",
    "            y_pred = model.forwardprop(data[\"input\"])\n",
    "            cost = cost_obj.cost(y_pred, data[\"label\"])\n",
    "            derive_loss = cost_obj.derivative_loss(y_pred, data[\"label\"])\n",
    "            model.backwardprop(derive_loss)\n",
    "            model.update_params(lr)\n",
    "            print(\"iteration {}, cost {}\".format(i, cost))\n",
    "    print(vanilla_gradient_descent(Model(64, 50), CrossEntropy(), weights_hidden, weights_output, data_dict, 0.001, 1000))\n",
    "\n",
    "            \n",
    "\n",
    "    # END YOUR CODE\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqPZMnjocYoj"
   },
   "source": [
    "The following helper function will be useful to initialize the weights arrays - that will be passed to the $initialize$-method of the <code>Model</code> class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wy6akHa2cYok"
   },
   "outputs": [],
   "source": [
    "def initial_weights(n0: int, n1: int):\n",
    "    \"\"\"\n",
    "    Prepares a random 2d numpy array of shape (n0,n1) suited as initial weights for the training of the neural network model.\n",
    "    \"\"\"\n",
    "    \n",
    "    weights_hidden = np.random.normal(size=(n1,n0), loc=0.0, scale=1.0/np.sqrt(n1*n0))\n",
    "    weights_output = np.random.normal(size=(1,n1), loc=0.0, scale=1.0/np.sqrt(n1))\n",
    "    return weights_hidden, weights_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqGM8crecYok"
   },
   "source": [
    "## Problem 4: Gradient Checking\n",
    "\n",
    "Implement the function `numerical_gradient` which computes the numerical gradient based on a small step size $\\varepsilon$. Use the formula \n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x_1,\\dots,x_n)}{\\partial x_k} \\approx \\frac{f(x_1,\\dots,x_k+\\varepsilon,\\dots,x_n) - f(x_1,\\dots,x_k-\\varepsilon,\\dots,x_n)}{2\\varepsilon}.\n",
    "$$\n",
    "\n",
    "Using the class implemented above, you have access to \n",
    "* your implementation of the gradient $\\frac{\\partial y^{\\rm (pred)}(x,\\Theta)}{\\partial \\Theta_k}$ \n",
    "* the model function $y^{\\rm (pred)}(x,\\Theta)$ that you can evaluate at different parameter values. Individually vary all single components by the parameter `eps`.\n",
    "\n",
    "You can set the parameter values by using the initialize method - by restricting to the _weights_ parameters of the model. This together with the `forwardprop` allows you to compute the model output $a^{\\rm (out)}$ for given parameters and $x$. Thereafter, you can compute  the derivatives with respect to the weights by using `backwardprop`. Note that, as $x$-values, you can use some arbitrary samples (random generated or from MNIST - see below). As derivatives w.r.t. to cost you can pass in just 1's.\n",
    "\n",
    "For which values of `eps` do we obtain a resonable numerical approxmiation of the gradient? Test different (not too large values) and use `matplotlib` for determining a resonable `eps`.  Compute the norm of the the difference vector between the numerical approximation and the analytically computed gradient vector (by using the rules of calculus). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3qqZvbXcYol"
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(model: Model,\n",
    "                       x: np.array,\n",
    "                       weights_hidden: np.array,\n",
    "                       weights_output: np.array,\n",
    "                       eps: float = 1e-5 \n",
    "                       ) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute for the given model, data point (x,y) and weights parameters (biases equal to zero) a numerical gradient approximation of the \n",
    "    gradient. \n",
    "    \n",
    "    Arguments:\n",
    "    model -- instance of the model class defined above\n",
    "    x -- data point (np.array with shape (ninput,))\n",
    "    weights_hidden -- weights matrix of shape (nhidden, ninput)\n",
    "    weights_output -- weights matrix of shape (noutput, nhidden)\n",
    "    eps -- value of epsilon (float)\n",
    "    \n",
    "    Returns:\n",
    "    gradient -- numerical gradient at the given weights parameters (for the hidden and the output layer)\n",
    "    \"\"\"\n",
    "    # START YOUR CODE\n",
    "\n",
    "\n",
    "\n",
    "    # END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9d7z39rAcYol"
   },
   "outputs": [],
   "source": [
    "# Comparison with the analytically computed gradient vector\n",
    "\n",
    "# START YOUR CODE\n",
    "\n",
    "\n",
    "\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpHWT8XFcYon"
   },
   "source": [
    "## MNIST Dataset\n",
    "\n",
    "We here want to train a classifier for the MNIST dataset that is often used in machine learning. The images are digitized, hand-written figures. Actually, we use a light-weight version which consists of only 1797 images with 8x8 pixels. The vectors contained in $X$ are 64-dim vectors (8x8=64) - its elements correspond to the gray-scale values of the pixels (normalized to be included in [0,1]). \n",
    "\n",
    "The label values are set to '1' (`true`) if the hand-written figure is a '1' and '0' (`false`) in all the other cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6A8yzXZHcYon",
    "outputId": "de430285-c33b-41b3-82f6-6616e7361e5f"
   },
   "outputs": [],
   "source": [
    "# Please make sure that the data file is included in the same folder as the given jupyter notebook. \n",
    "data = np.load( \"./mnist.npz\" )\n",
    "X = data['X'].T[1:,:]\n",
    "y = data['y'].T.reshape(1,X.shape[1])\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MU1APPx9cYoo"
   },
   "source": [
    "Now, you can access the samples of the dataset by accessing the columns of $X$. In numpy: `x_i = X[:,i]`. \n",
    "If you reshape it to 8x8 you can eventually recognize the digit depicted on the image (a '0' in the example below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hCJ0OpfcYoo",
    "outputId": "eeb108e2-b60f-4a46-d6f6-827abce6ce12"
   },
   "outputs": [],
   "source": [
    "print(X[:,0])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RczSG4JIcYop"
   },
   "source": [
    "The 2d arrays can be plotted as image by applying the following code snippets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sfmYKitcYop",
    "outputId": "3bf71f96-a4cf-48ec-96b0-0f5ff82bad95"
   },
   "outputs": [],
   "source": [
    "item = 0\n",
    "plt.imshow( X[:,item].reshape(8,8), cmap=\"binary\" )\n",
    "plt.title( \"Label: \" + str( y[0,item] )) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNPbS4bhcYoq"
   },
   "source": [
    "The following should rather be a '1' (since the label is '1'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIHYG3oJcYoq",
    "outputId": "c3503bb8-d584-4344-aec7-ed2b4499f6fc"
   },
   "outputs": [],
   "source": [
    "item = 1\n",
    "plt.imshow( X[:,item].reshape(8,8), cmap=\"binary\" )\n",
    "plt.title( \"Label: \" + str( y[0,item] )) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vuB2buTcYoq"
   },
   "source": [
    "## Problem 5: Apply GD to MNIST\n",
    "\n",
    "Apply the gradient descent to the model defined above. We here ignore ML specific aspects such as using a part of the data \n",
    "for validation and test or dealing with class-imbalance. We rather just want to see whether the optimization works.  \n",
    "\n",
    "(1) Apply the functionality implemented above to train MNIST. Use the <code>learningcurve_plots</code> function to visualize whether gradient descent was performing well over the iterations. Also compute the _final error rate_, i.e. the percentage of wrong predictions. The predicted label can be obtained from $y^{(pred)}(\\theta,x)$ by just rounding it to 0 or 1.\n",
    "\n",
    "(2) Compare and characterize the shape of the learning for different learning rates. What's the \"best\" learning rate (<code>lr</code>). How many iterations (<code>epochs</code>) should be set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsr1_VzqcYor",
    "outputId": "24a8a9cb-ce7c-4a6a-ca86-6c1130131844"
   },
   "outputs": [],
   "source": [
    "n0 = X.shape[0]\n",
    "n1 = 50\n",
    "\n",
    "lr = 6\n",
    "epochs = 2000\n",
    "print(\"Learning Rate: %6.5f\"%lr)\n",
    "print(\"Epochs: %6.5f\"%epochs)\n",
    "\n",
    "# START YOUR CODE\n",
    "\n",
    "\n",
    "# END YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FufXF3cicYor"
   },
   "source": [
    "# Cat/NoCat Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fkd5uOzFcYos"
   },
   "source": [
    "You now can apply the approach(es) implemented above to the cat/nocat use case - alternatively, you can pick another suitable use case. \n",
    "\n",
    "For loading the cat/nocat dataset run the following lines of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjRP7iMpcYos",
    "outputId": "985ab5c6-27bd-4bd1-964f-df229615d3e4"
   },
   "outputs": [],
   "source": [
    "cat_nocat_data = np.load(\"./cat_nocat.npz\") # point to the suitable folder relative to the folder of the notebook\n",
    "X_train = cat_nocat_data['X_train'].T\n",
    "X_test = cat_nocat_data['X_test'].T\n",
    "y_train = cat_nocat_data['y_train'].T.reshape(1,X_train.shape[1])\n",
    "y_test = cat_nocat_data['y_test'].T.reshape(1,X_test.shape[1])\n",
    "[X_train.shape, y_train.shape, X_test.shape, y_test.shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GG1sYauwcYot"
   },
   "source": [
    "With the following function you can plot a few sample images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Eig-vTZcYot",
    "outputId": "36ce533e-62aa-40cb-eb55-e4550ee85379"
   },
   "outputs": [],
   "source": [
    "def plot_image( X, y ):\n",
    "    plt.imshow( X.reshape(64,64,3) )\n",
    "    title = 'cat' if y==1 else 'non-cat'\n",
    "    plt.title( title )\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        plt.subplot2grid((10,10),(i,j))\n",
    "        plot_image( X_train[:,10*i+j], y_train[0,10*i+j] )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8H6_NUdcYou"
   },
   "source": [
    "Actually, we have loaded \n",
    "* a training set (with 209 samples) that can be used to train the model, i.e. optimize the model parameters\n",
    "* a test set (with 50 samples) that can be used to test the performance of the trained model.\n",
    "\n",
    "The images are flattened arrays of size $64\\times 64\\times 3=12288$, i.e. quite large arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBzicjv2cYou"
   },
   "source": [
    "## Problem 6 : Apply GD to Cat/NoCat Dataset\n",
    "\n",
    "Apply Gradient Descent to train a classifier that can distinguish images with or without cats. Reuse the functionality implemented in Problem 5.\n",
    "\n",
    "(1) Optimize the defined cost function with `X_train` und `y_train`. Again, use <code>learningcurve_plots</code> to visualize whether gradient descent was performing well over the iterations. The training will take more time than for MNIST. So, at beginning use at max 1000 Iterationen.\n",
    "\n",
    "(2) Use the trained model to make predictions for the test data (not used for training). Determine the percentage of correct predictions for the train and test dataset (alternatively the error rate). As label predicted by the model you can use (for simplicity) $y^{(pred)}(\\theta, x)$ round to 0 or 1. \n",
    "\n",
    "(3) What is a reasonable number of iterations (epochs) to be chosen?\n",
    "\n",
    "(4) Explore the images for which the model gives a wrong prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLkVZvVIcYou",
    "outputId": "6b803c91-ee8a-46e9-ba23-95cc0872ef0a"
   },
   "outputs": [],
   "source": [
    "n0 = X_train.shape[0]\n",
    "n1 = 50\n",
    "\n",
    "lr = 0.02\n",
    "epochs = 1000\n",
    "print(\"Learning Rate: %6.5f\"%lr)\n",
    "print(\"Epochs: %6.5f\"%epochs)\n",
    "\n",
    "\n",
    "# START YOUR CODE\n",
    "\n",
    "\n",
    "\n",
    "# END YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4g2Ax84ZcYov"
   },
   "source": [
    "### Inspect where Classifier is confused\n",
    "\n",
    "i.e. where the trained classifier makes the wrong prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emkP2xtTcYow"
   },
   "outputs": [],
   "source": [
    "ypred = model.forwardprop(X_test)\n",
    "indices = (np.round(ypred).astype(int) != y_test)[0,:]\n",
    "confused = X_test[:,indices]\n",
    "label = y_test[:,indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6s4SJ4FPcYow",
    "outputId": "a1e703d9-e25f-4bd2-9a6c-f5bd35efdaf7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        plt.subplot2grid((10,10),(i,j))\n",
    "        plot_image( confused[:,4*i+j], label[0,4*i+j] )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPNPxlHjcYox"
   },
   "outputs": [],
   "source": [
    "ypred = model.forwardprop(X_test)\n",
    "indices = (np.round(ypred).astype(int) == y_test)[0,:]\n",
    "correct = X_test[:,indices]\n",
    "label = y_test[:,indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxIQ3Sj_cYox",
    "outputId": "88cefc69-c710-4481-c48a-fa83bcc444a6"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        plt.subplot2grid((10,10),(i,j))\n",
    "        plot_image( correct[:,4*i+j], label[0,4*i+j] )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pux71vMtcYox"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "train_neural_net_stud_V4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
